{"version":"1","records":[{"hierarchy":{"lvl1":"Computer representation of numbers"},"type":"lvl1","url":"/float","position":0},{"hierarchy":{"lvl1":"Computer representation of numbers"},"content":"#%config InlineBackend.figure_format = 'svg'\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nWe are used to decimal system for counting. In general we can use any base β for the number system. On most modern computers, the base \\beta = 2, i.e., we use the binary number system. Any non-zero number x is written asx = s \\cdot (.a_1 a_2 \\ldots a_t)_\\beta \\cdot \\beta^e = \\textrm{(sign)} \\cdot\n\\textrm{(fractional part)} \\cdot \\textrm{(exponent)}\n\nwhere\\begin{aligned}\ns = -1 \\textrm{ or } +1 \\\\\n0 \\le a_i \\le \\beta - 1 \\\\\nL \\le e \\le U \\textrm{ is an integer}\n\\end{aligned}\n\nIn base 10, the fractional part has the value(.a_1 a_2 \\ldots a_t)_\\beta = \\frac{a_1}{\\beta} + \\frac{a_2}{\\beta^2} + \\ldots +\n\\frac{a_t}{\\beta^t}\n\nWe will assume that a_1 \\ne 0 which is called a normalized floating point system and then1 \\le a _i \\le \\beta-1, \\qquad i=1,2,\\ldots,t\n\n(\\beta, t, L,U) specifies the arithmetic characteristics of the floating point system.\n\n","type":"content","url":"/float","position":1},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Chopping and rounding"},"type":"lvl2","url":"/float#chopping-and-rounding","position":2},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Chopping and rounding"},"content":"Most real numbers cannot be exactly represented on a computer since we have finite precision. Hence they must be represented by a nearby number in the floating point system. This requires us to truncate a real number to fit into the floating point system, which can be done in two ways: chopping and rounding.\n\nConsider a real numberx = s \\cdot (.a_1 a_2 \\ldots a_t a_{t+1} \\ldots )_\\beta \\cdot \\beta^e, \\qquad a_1 \\ne 0\n\nLet\\fl{x} = \\textrm{approximation of $x$ in the floating point system}\n\nchopped machine representation\\fl{x} = s \\cdot (.a_1 a_2 \\ldots a_t)_\\beta \\cdot \\beta^e\n\nWe throw away all digits in the fractional part that do not fit into the floating-point system.\n\nrounded machine representation\\fl{x} = \\begin{cases}\ns \\cdot (.a_1 a_2 \\ldots a_t)_\\beta \\cdot \\beta^e & 0 \\le a_{t+1} < \\half\\beta \\\\\ns \\cdot [(.a_1 a_2 \\ldots a_t)_\\beta + (.0\\ldots01)_\\beta ]\\cdot \\beta^e & \\half\\beta \\le\na_{t+1} < \\beta\n\\end{cases}\n\nWe approximate by the nearest floating point number.\n\n","type":"content","url":"/float#chopping-and-rounding","position":3},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Error in computer representation"},"type":"lvl2","url":"/float#error-in-computer-representation","position":4},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Error in computer representation"},"content":"For most real numbers x \\ne \\fl{x}. Define the relative error\\epsilon = \\epsilon(x) = \\frac{\\fl{x} - x}{x}\n\nWe have following bounds on the relative error\n\nChopping-\\beta^{-t + 1} \\le \\epsilon \\le 0\n\nRounding-\\half \\beta^{-t + 1} \\le \\epsilon \\le \\half \\beta^{-t+1}\n\nThus the floating point approximation \\fl{x} of x \\in \\re satisfies\\fl{x} = x (1 + \\epsilon)\n\nThis type of analysis was introduced by Wilkinson (1963). It allows us to deal precisely with the effects of rounding/chopping in computer arithmetic operations.\n\n","type":"content","url":"/float#error-in-computer-representation","position":5},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Proof for chopping"},"type":"lvl2","url":"/float#proof-for-chopping","position":6},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Proof for chopping"},"content":"Take sign s=+1. Then \\fl{x} \\le x andx - \\fl{x} = (.00\\ldots 0a_{t+1}\\ldots)_\\beta \\cdot \\beta^e\n\nLet \\gamma=\\beta-1 (largest digit in base β). Then0 & \\le x - \\fl{x} \\\\\n& \\le (.00\\ldots 0 \\gamma\\gamma\\ldots)_\\beta \\cdot \\beta^e \\\\\n& = \\gamma \\left[ \\frac{1}{\\beta^{t+1}} + \\frac{1}{\\beta^{t+2}} + \\ldots \\right]\n\\beta^e \\\\\n& = \\gamma \\frac{\\beta^{-t-1}}{1- \\beta^{-1}} \\beta^e \\\\\n& = \\beta^{-t + e}\n\nThe relative error is0 &\\le \\frac{x - \\fl{x}}{x} \\\\\n&\\le \\frac{\\beta^{-t+e}}{(.a_1a_2\\ldots)_\\beta \\cdot \\beta^e} \\\\\n&\\le \\frac{\\beta^{-t}}{(.100\\ldots)_\\beta} \\\\\n&= \\beta^{-t+1}\n\n","type":"content","url":"/float#proof-for-chopping","position":7},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Accuracy of floating numbers: Unit Round"},"type":"lvl2","url":"/float#accuracy-of-floating-numbers-unit-round","position":8},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Accuracy of floating numbers: Unit Round"},"content":"The unit round \\uround\n\nIt is a positive floating point number.\n\nIt is the smallest such number for which\n\n\\fl{1 + \\uround} > 1\n\nThis means that for any \\delta < \\uround, then\\fl{1 + \\delta} = 1\n\n1 and 1 + \\delta are identical within the computer arithmetic. Thus the unit round characterizes the accuracy of arithmetic computations. We have\\uround = \\begin{cases}\n\\beta^{-t+1} & \\textrm{for chopping} \\\\\n\\half\\beta^{-t+1} & \\textrm{for rounding}\n\\end{cases}\n\nHence we get the important relation for floating point numbers\\boxed{\\fl{x} = x(1 + \\epsilon), \\qquad |\\epsilon| \\le \\uround}\n\nWith rounding on a binary computer, show that \\uround = 2^{-t}\n\nWe must show that\\fl{1 + 2^{-t}} > 1\n\nFirstly 2^{-t} = (.10\\ldots 0)_2 \\cdot 2^{-t+1} is a floating point number.1 + 2^{-t} &= [ (.10\\ldots 0)_2 + (.00\\ldots 010\\ldots)_2] \\cdot 2^1 \\\\\n&= (.10\\ldots 01)_2 \\cdot 2^1 \\qquad \\textrm{(1 at  $(t+1)$'th position})\n\nHence rounding gives\\fl{1 + 2^{-t}} =& (.10\\ldots 10)_2 \\cdot 2^1 \\\\\n& \\textrm{rounding gives 1 at $t$'th position} \\\\\n=& 1 + 2^{-t+1} \\\\\n>& 1\n\nIf \\delta < \\uround, then1 + \\delta =& [ (.10\\ldots 0)_2 + (.00\\ldots 00\\ldots)_2] \\cdot 2^1 \\\\\n& \\qquad\\qquad \\textrm{zero in $(t+1)$'th position} \\\\\n=& (.10\\ldots 00\\ldots)_2 \\cdot 2^1\n\nHence, rounding to t places gives\\fl{1+\\delta} = (.10\\ldots 0)_2 \\cdot 2^1 = 1\n\nThe following code finds the unit round by finding the smallest i such that 1 + 2^{-i} = 1.\n\ni, x = 0, 1.0\nwhile 1.0 + x > 1.0:\n    x = x / 2\n    i = i + 1\nprint(i)\n\nThis shows that in double precision, 1 + 2^{-53} = 1, and thus the unit round is \\uround = 2^{-53}.\n\n","type":"content","url":"/float#accuracy-of-floating-numbers-unit-round","position":9},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Underflow and overflow"},"type":"lvl2","url":"/float#underflow-and-overflow","position":10},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Underflow and overflow"},"content":"The smallest positive floating point number in the system (\\beta,t,L,U) isx_L = (.10\\ldots.0)_\\beta \\cdot \\beta^L = \\beta^{L-1}\n\nand the largest positive floating point number isx_U = (.\\gamma\\gamma\\ldots\\gamma)_\\beta \\cdot \\beta^U = (1-\\beta^{-t}) \\beta^U\n\nThe range of numbers that are representable in the floating-point system isx_L \\le |x| \\le x_U\n\nObviously, we cannot represent a number which is outside this range.\n\nOverflow error: |x| > x_U\n\nleads to fatal error in computation\n\nprogram may terminate\n\nUnderflow error: |x| < x_L\n\nmay not be a fatal error\n\n\\fl{x} is set to zero and computations can continue\n\nHowever, in some cases, accuracy may be severly lost.\n\n","type":"content","url":"/float#underflow-and-overflow","position":11},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"IEEE floating point system"},"type":"lvl2","url":"/float#ieee-floating-point-system","position":12},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"IEEE floating point system"},"content":"The IEEE floating point system was developed under the leadership of Prof. William Kahan of Univ. of California, Berkely.  Project 754 was formed in 1970 to produce the best possible definition of floating-point arithmetic. The IEEE 754 Standard gives\n\ndefinition of floating-point numbers\n\nrounding operations\n\nformat conversion\n\nexception rules (invalid operation, division by zero, overflow, underflow)","type":"content","url":"/float#ieee-floating-point-system","position":13},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl3":"IEEE single precision","lvl2":"IEEE floating point system"},"type":"lvl3","url":"/float#ieee-single-precision","position":14},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl3":"IEEE single precision","lvl2":"IEEE floating point system"},"content":"Fortran77: real*4, Fortran90: real, C/C++: float\n\n32 bit word\n\n1 bit for sign\n\n8 bits for biased exponent (L=-126, U=127); 2^8 = 256 = 126 + 1 + 127 + 1 + 1, the last two are for Inf and NaN.\n\n23 bits for fractional part (mantissa)\n\nLargest number = (2 - 2^{-23}) \\cdot 2^{127} \\approx 3.4028 \\times 10^{38}\n\nSmallest positive normalized number = 2^{-126} \\approx 1.1755 \\times 10^{-38}\n\nUnit roundoff, \\uround = 2^{-24} \\approx 5.96 \\times 10^{-8}, corresponds to about seven decimal places.","type":"content","url":"/float#ieee-single-precision","position":15},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl3":"IEEE double precision","lvl2":"IEEE floating point system"},"type":"lvl3","url":"/float#ieee-double-precision","position":16},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl3":"IEEE double precision","lvl2":"IEEE floating point system"},"content":"Fortran77: real*8, Fortran90: double precision, C/C++: double\n\nBased on two 32 bit words or one 64 bit word\n\n1 bit for the sign\n\n11 bits for the biased exponent (L=-1022, U=1023)\n\n52 bits for the fractional part\n\nLargest number = (2 - 2^{-52}) \\cdot 2^{1023} \\approx 1.7977 \\times 10^{308}\n\nSmallest positive normalized number = 2^{-1022} \\approx 2.2251 \\times 10^{-308}\n\nUnit round \\uround = 2^{-53} \\approx 1.11 \\times 10^{-16}\n\n","type":"content","url":"/float#ieee-double-precision","position":17},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Precision of computer arithmetic and physics"},"type":"lvl2","url":"/float#precision-of-computer-arithmetic-and-physics","position":18},{"hierarchy":{"lvl1":"Computer representation of numbers","lvl2":"Precision of computer arithmetic and physics"},"content":"With IEEE double precision, the interval [1,2] is approximated by about \n\n1016 floating point numbers. In a handful of solid or liquid, or a balloon of gas, the number of atoms/molecules in a line from one point to another is of the order of \n\n108 (cube root of Avogadro number). Such a system behaves like a continuum, enabling us to define physical quantities like density, pressure, stress, strain and temperature.  Computer arithmetic is more than a million times finer tham this.\n\nThe fundamental constants of physics are known to only a few decimal places.\n\ngravitational constant G - 4 digits\n\nPlanck’s constant h - 7 digits\n\nelementary charge e - 7 digits\n\nratio of magnetic moment of electron to the Bohr magneton \\mu_e/\\mu_B - 12 digits\n\nNothing in physics is known to more than 12 or 13 digits of accuracy. IEEE numbers are orders of magnitude more precise than any number in science. Mathematical quantities like π are of course known to more accuracy.\n\nIn the early days of numerical analysis, computers were not very precise, numerical algorithms, especially conditioning and stability were not well understood. Hence there was too much emphasis on studying rounding errors. Today, computer arithmetic is more precise and well understood, and we can control rounding errors through stability of numerical algorithms.\n\nThe main business of numerical analysis is designing algorithms that converge quickly; rounding error analysis is rarely the central issue. If rounding errors vanished, 99% of numerical analysis would remain. - Lloyd N. Trefethen\n\nRound-off errors\n\nEvaluatef(x) = 1 - \\cos x\n\nfor x = 10^{-8}. This yields f = 0.0 even in double precision.\n\nfrom math import cos\nx = 1.0e-8\ny = 1.0 - cos(x)\nprint(\"%24.14e\" % y)\n\nAn equivalent expression isf(x) = 2 \\sin^2(x/2)\n\nwhich yields f = 5 \\times 10^{-17}.\n\nfrom math import sin\nz = 2.0*sin(0.5*x)**2\nprint(\"%24.14e\" % z)\n\nThe first form suffers from roundoff errors while the second one is more accurate. This may or may not be a problem. A situation where this is problematic is if you want to computef(x) = \\frac{1 - \\cos(x)}{x^2}\n\ny = (1.0 - cos(x))/x**2\nprint(\"%24.14e\" % y)\n\nWe will get an answer of zero, but the correct value is closer to \\half.\n\nz = 2.0*sin(0.5*x)**2 / x**2\nprint(\"%24.14e\" % z)\n\nRound-off error\n\nConsider computingx = 10^{-8}, \\qquad y = \\sqrt{1+x^2} - 1\n\nfrom math import sqrt\nx = 1.0e-8\ny = sqrt(1.0 + x**2) - 1.0\nprint(\"%20.10e\" % y)\n\nThe result is zero due to round-off error. An equivalent expression isy = \\frac{x^2}{\\sqrt{1 + x^2} + 1}\n\ny = x**2/(sqrt(1.0+x**2) + 1.0)\nprint(\"%20.10e\" % y)\n\nThis is more accuratey = \\sqrt{1 + x^2} - 1 \\approx (1 + x^2/2)-1 = \\frac{1}{2}x^2 = 5.0 \\times 10^{-17}\n\npolynomial evaluation\n\nLet us evaluatey = x^3 - 3 x^2 + 3 x - 1\n\nin single precision. Note that it can also be written asy = (x-1)^3\n\nx = np.linspace(0.99,1.01,50,dtype=np.float32)\ny = x**3 - 3.0*x**2 + 3.0*x - 1.0\nplt.plot(x,y,'-o',x,(x-1)**3)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(('Single precision','Exact'));\n\nNow, let us do it in double precision.\n\nx = np.linspace(0.99,1.01,50,dtype=np.float64)\ny = x**3 - 3.0*x**2 + 3.0*x - 1.0\nplt.plot(x,y,'-o',x,(x-1)**3)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend(('Double precision','Exact'));\n\nThe functionf(x) = coth(x) - 1/x\n\nis finite at x=0 but the two terms are not. Note thatf(x) = \\frac{x}{3} - \\frac{x^3}{45} + \\frac{2 x^5}{945} + \\ldots\n\nApproximate this in two parts [0,\\delta] and [\\delta,\\infty). How to choose δ ?","type":"content","url":"/float#precision-of-computer-arithmetic-and-physics","position":19},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/intro","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"Numerical Analysis is concerned with the construction and analysis of algorithms to solve a mathematically posed problem on a computer so as to generate a numerical answer.  This is necessary because most mathematical equations do not have closed form solutions that can be easily evaluated. Even in cases where a closed form solution exists, it may be necessary to resort to a computer to obtain a numerical answer. The mathematical problems that we want to solve numerically arise from science and engineering, where a physical system is modeled mathematically in terms of equations. These equations may be algebraic, ordinary or partial differential equations, or a combination of these. Their solutions are functions in one or many variables, e.g., space and/or time. We assume that space and time are a continuum and use real numbers to represent them. However, the real numbers form an infinite set and we cannot hope to represent them in a computer which has finite memory and resources. On a computer, real numbers are approximated by a finite set of floating point numbers.\n\n","type":"content","url":"/intro","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Example: Approximation of functions"},"type":"lvl2","url":"/intro#example-approximation-of-functions","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Example: Approximation of functions"},"content":"A general function f : [a,b] \\to \\re has an infinite amount of information which we cannot represent on a computer. For most applications it is enough to obtain an approximation to the function.\n\nGiven a function f \\in V we want to find a function f_n \\in V_n where V_n is finite\ndimensional, such that\\| f - f_n \\| = \\inf_{g \\in V_n} \\| f - g \\|\n\nThe norm is usually maximum norm or L^2 norm, leading to uniform approximation and least squares approximation.\n\nThe above kind of approximation requires knowledge of the full function which may not be available. We can sample the function at a set of n discrete pointsa=x_0 < x_1 < \\ldots < x_{n-1} = b\n\nand construct an approximation f_n(x) to f(x), for example by interpolationf_n(x_i) = f(x_i), \\qquad i=0,1,\\ldots,n-1\n\nUsually f_n \\in V_n is a polynomial of some specified degree since these are easy to evaluate on a computer. The approximation f_n is something we can represent in a computer because it has finite amount of information, and we can evaluate it at some unknown x and hope that f_n(x) \\approx f(x). As we let n \\to \\infty, we hope that f_n \\to f in some suitable norm.\n\nInterpolation tries to exactly match the approximation to the true value, which may not be a good idea if the data contains noise. Instead of interpolation, we can perform a least squares fit\\|f - f_n\\|_n = \\min_{g \\in V_n} \\| f - g \\|_n, \\quad \\| f - f_n \\|_n^2 = \\sum_{i=0}\n^{n-1}[ f(x_i) - f_n(x_i)]^2\n\nSome typical questions to ask:\n\nHow to choose the spaces V_n ? For interpolation this includes the choice of the nodes. Polynomials, rational polynomials and trigonometric functions are the most common choices for the function spaces.\n\nWhat is the error \\| f - f_n \\| ?\n\n","type":"content","url":"/intro#example-approximation-of-functions","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Example: Differential equations"},"type":"lvl2","url":"/intro#example-differential-equations","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Example: Differential equations"},"content":"In many problems, the function that we seek may be unknown and it may be the solution of some differential equation: find f \\in V such thatL(f) = 0\n\nwhere L is a differential operator and V is some infinite dimensional function space.  Such a problem is replaced by a finite dimensional problem: find f_n \\in V_n such thatL_n(f_n) = 0\n\nwhere V_n is a finite dimensional space of functions and L_n is an approximation of the differential operator L. The space V_n can be constructed by an interpolation process as in the previous example. We would like to show that f_n \\to f as n \\to \\infty.\n\nAs an example consider-u''(x) = f(x), \\qquad x \\in (0,1)\n\nwith boundary conditionsu(0) = u(1) = 0\n\nIn the finite difference method, we divide the domain with grid points x_i = ih, i=0,1,\\ldots,n-1, h = 1/(n-1) and replace derivatives with finite differencesu_0 = & 0 \\\\\n- \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} = & f_i, \\qquad 1 \\le i \\le n-2 \\\\\nu_{n-1} = & 0\n\nThis is a system of linear equations which can be put in matrix formAU = b\n\nwhere A is a tri-diagonal matrix. The numerical solution of differential equations can\nthus lead to matrix equations.\n\n","type":"content","url":"/intro#example-differential-equations","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"Example: Roots, matrix equations"},"type":"lvl2","url":"/intro#example-roots-matrix-equations","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"Example: Roots, matrix equations"},"content":"Suppose we want to find the roots of a function f : \\re^n \\to \\re^n, i.e., find r \\in \\re^n such that f(r) = 0. Or we\nmay want to find the solution of Ax=b where A is a n \\times n matrix and b\nis an n-vector. Usually such problems arise as a consequence trying to numerically\nsolve a differential equation, e.g., for a linear PDE we may end up withL_n(f_n) = A F - b = 0\n\nwhere F \\in \\re^n is the set of function values at the n distinct nodes. To solve such\nproblems we construct algorithms which are iterative in nature, i.e., given an initial\nguess F^0, the algorithm updates it in a series of steps,F^{k+1} = H(F^k), \\qquad k=0,1,2,\\ldots\n\nand we hope that as k \\to \\infty we approach closer to the solution, i.e., F^k \\to F =\nA^{-1}b. Note that the solution is a fixed point of H.\n\nFor solving matrix equations, we can also employ direct methods like Gaussian elimination\nwhich give the answer in a finite number of steps. However such methods may be\nlimited to small problem sizes.","type":"content","url":"/intro#example-roots-matrix-equations","position":7},{"hierarchy":{"lvl1":"Introduction","lvl2":"Example: Integration"},"type":"lvl2","url":"/intro#example-integration","position":8},{"hierarchy":{"lvl1":"Introduction","lvl2":"Example: Integration"},"content":"Given a function f : [a,b] \\to \\re, we want to compute a numerical value of the integralI(f) = \\int_a^b f(x) dx\n\nWe will construct an approximation of the formI_n(f) = \\sum_{j=0}^{n-1} w_j f(x_j)\n\nwhere n is an integer, w_j are some quadrature weights and x_j are corresponding\nnodes. Typical questions to ask are:\n\nWhat is the error in the approximation I(f) - I_n(f) ?\n\nGiven n, how to choose the weights and nodes to get the best possible approximation ?\n\n","type":"content","url":"/intro#example-integration","position":9},{"hierarchy":{"lvl1":"Introduction","lvl2":"Aims of Numerical Analysis"},"type":"lvl2","url":"/intro#aims-of-numerical-analysis","position":10},{"hierarchy":{"lvl1":"Introduction","lvl2":"Aims of Numerical Analysis"},"content":"Numerical Analysis is concerned with constructing algorithms to solve such problems and further we would like to analyze these algorithms in terms of the following questions.\n\nAccuracy: How well does the numerical solution approximate the exact solution ?\n\nConvergence: As we refine the discretization, does the approximation converge to the true solution ? In case of iterative methods, do the iterations converge ?\n\nConvergence rate, efficiency: How fast does it converge ? E.g., we may have\\| f_n - f \\| \\le C n^{-\\alpha}, \\qquad \\alpha > 0\n\nor better still\\| f_n - f \\| \\le C \\exp(-\\alpha n), \\qquad \\alpha > 0\n\nFor iterative scheme, we would like\\| F^{k+1} - F \\| \\le \\alpha \\| F^k - F \\|, \\qquad 0 < \\alpha < 1\n\nor better still\\| F^{k+1} - F \\| \\le \\alpha \\| F^k - F \\|^p, \\qquad \\alpha > 0, \\quad p > 1\n\nStability: Is the numerical algorithm stable ? If we change the data by a small\namount, we hope that the answer given by the algorithm changes by a small amount.\n\nBackward stability: Show that the approximate solution to some problem, is the\nexact solution of a nearby problem. E.g., if x^* is an approximate solution to\nAx=b, show that it is the exact solution to (A+E)x=b where E is small.","type":"content","url":"/intro#aims-of-numerical-analysis","position":11},{"hierarchy":{"lvl1":"Root finding"},"type":"lvl1","url":"/roots","position":0},{"hierarchy":{"lvl1":"Root finding"},"content":"Given a continuous function f : [a,b] \\to \\re, find an \\alpha \\in [a,b] such that\nf(\\alpha) = 0. Such an α is called a root of f or a zero of f. There could\nbe many roots for a given function. At a root, the positive and negative parts of f\ncancel one another. So we have to careful about roundoff errors and care must be\ntaken to evaluate the function so as to avoid loss of significance errors. Since we work\nwith finite precision on the computer, we cannot expect to find an α that\nmakes the function exactly zero. At best, we hope to keep relative error under\ncontrol. In this sense, zeros far from the origin cannot be computed with great\nabsolute accuracy. Since the purpose is to solve some problem that involves\napproximations, it is not necessary to exactly locate the root, and an approximation is\nsufficient.\n\nMinima of a function\n\nTo find the location where a given function\nf : \\re^N \\to \\re attains a local minimum, we have to solve the equation f'(x) =\n0. Note that g = f' : \\re^N \\to \\re^N and we have a multi-dimensional root finding\nproblem: find x = (x_1, x_2, \\ldots, x_N) such thatg_i(x_1, x_2, \\ldots, x_N) = \\df{f}{x_i}(x_1, x_2, \\ldots, x_N) = 0, \\qquad 1 \\le i \\le N\n\nSolving ODE\n\nConsider the system of ODE\\dd{u(t)}{t} = f(u(t)), \\qquad t \\in [0,T]\n\nwhere u \\in \\re^N and f : \\re^N \\to \\re^N. We are given some initial condition u(0) = u_0 \\in \\re^N. The backward Euler scheme is given by\\frac{u^{n+1} - u^n}{\\Delta t} = f(u^{n+1})\n\nThis is a set of N non-linear equations for the unknown solution u^{n+1}. We can defineH(u) = \\frac{u - u^n}{\\Delta t} - f(u)\n\nThe solution u^{n+1} we seek is the root of the function H : \\re^N \\to \\re^N. We have to solve a root problem for every time interval.\n\n","type":"content","url":"/roots","position":1},{"hierarchy":{"lvl1":"Root finding","lvl2":"Bracketing the root"},"type":"lvl2","url":"/roots#bracketing-the-root","position":2},{"hierarchy":{"lvl1":"Root finding","lvl2":"Bracketing the root"},"content":"There are no general methods to find the roots of an arbitrary function. The user must have some knowledge of where the root might possibly lie. It is hence useful to first find some small interval in which we expect the root to lie.\n\nIntermediate Value Theorem\n\nIf f \\in \\cts[a,b] and y lies between f(a) and f(b), then there is atleast one point x \\in [a,b] such that y = f(x).\n\nAround a root, the function takes both positive and negative values. The IVT can be used to find an interval containing the root.\n\nBracketing interval\n\nIf \\sign f(a) \\ne \\sign f(b), then there is alteast one root of f in the interval [a,b].\n\n","type":"content","url":"/roots#bracketing-the-root","position":3},{"hierarchy":{"lvl1":"Root finding","lvl2":"Bisection method"},"type":"lvl2","url":"/roots#bisection-method","position":4},{"hierarchy":{"lvl1":"Root finding","lvl2":"Bisection method"},"content":"Let f be a continuous function. Given an interval [a,b] such that\\sign f(a) \\ne \\sign f(b), \\qquad f(a) \\ne 0 \\ne f(b)\n\nthen [a,b] is called a {\\em non-trivial bracket} for a root of f. The basic idea of bisection method is: starting from a non-trivial bracket, find a new bracketing interval which is smaller in size. As the name suggests, we divide the interval into two equal and smaller intervals. First divide [a,b] into [a,c] and [c,b] where c = \\half(a+b). Then there are three possibilities.\n\nf(c) = 0, then we have found the root exactly.\n\nf(c) \\ne 0 and \\sign f(c) \\ne \\sign f(b). Then [c,b] is a non-trivial bracket.\n\nf(c) \\ne 0 and \\sign f(a) \\ne \\sign f(c). Then [a,c] is a non-trivial bracket.\n\nWe repeat the above process until the root is found or the length of the bracketing interval is reduced to a small desired size. The method may not give a unique value for the root since it can lie anywhere in the bracketing interval. We can take the mid-point of the bracketing interval as the estimate of the root, in which case the error in the root is at most equal to half the length of the bracketing interval.\n\nThe length of the bracketing interval reduces by half in each iteration, which means that the algorithm has a monotonic convergence behaviour. IfL_0 = b-a = \\textrm{initial length}\n\nthen after k iterationsL_k = \\frac{1}{2^k} L_0\n\nAs a convergence criterion, we can put a tolerance on the length: stop if L_k \\le \\epsilon for some \\epsilon > 0 small. To achieve this tolerance, we requirek \\approx \\log_2 \\frac{L_0}{\\epsilon}\n\nIf L_0 = 1 and \\epsilon = 10^{-6} then we need about 20 iterations. If we demand more accuracy, then the number of iterations will increase, though only logarithmically. The bisection method is shown in Algorithm~(1). We have put an upper limit on the number of iterations and we also specify the stopping criterion in terms of the length of the bracketing interval and/or the function value.","type":"content","url":"/roots#bisection-method","position":5}]}